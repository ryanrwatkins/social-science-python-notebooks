{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License © 2019 R. Watkins\n",
    "\n",
    "Note: This tutorial is based on: https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Web scraping is a useful technique to convert unstructured data on the web (such as a table or list) to structured data (such as dataframe) that you can use for a variety of purposes. For example, you might want to scrape an education website to get data on school test scores, or you might want to take voting information by zip code from a government webpage to create a visual image of voting trends in your area.\n",
    "\n",
    "These are just a few of the questions / problems / products whose solutions might start with web scraping and information extraction (data collection) before you get to data analysis and interpretation.\n",
    "\n",
    "\n",
    "## Ways to extract information from web\n",
    "\n",
    "There are several ways to extract information from the web. **APIs** are probably the best way to extract data from a website. Almost all large websites like Twitter, Facebook, Google, Reddit, StackOverflow provide APIs to access their data (or at least limited sections of their data) in a more structured manner. If you can get the data you want through an API, it is almost always preferred approach over web scraping. This is because if you are getting access to structured data from the provider, why would you want to create an engine to extract the same information.\n",
    "\n",
    "**RSS feeds** are another way that a website can share information for people to use for other purposes. For example, blogs will often produce an RSS feed that you can use to get a copy of all the recent posts, which you can then for your work.  But they are limited in their use and are mostly found in sites that have routine updates (such as news pages, or podcasts).\n",
    "\n",
    "But what can you do when you want information that is on a website but they don't have an API or RSS feed that meets your purposes? Well, that is when you scrape the website to fetch the information.\n",
    "\n",
    "\n",
    "## What is Web Scraping?\n",
    "\n",
    "Web scraping is a technique for extracting information from websites. This technique mostly focuses on the transformation of unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n",
    "\n",
    "You can perform web scraping in various ways, including use of Google Docs to almost every programming language. I would resort to Python because of its ease and rich ecosystem. It has a library known as ‘BeautifulSoup’ which assists this task. In this article, I’ll show you the easiest way to learn web scraping using python programming.\n",
    "\n",
    "For those of you, who need a non-programming way to extract information out of web pages, you can also look at import.io . It provides a GUI driven interface to perform all basic web scraping operations. The hackers can continue to read this article!\n",
    "\n",
    " \n",
    "\n",
    "## Libraries required for web scraping\n",
    "Python is an open source programming language and you will often find multiple libraries that can perform the same function. Hence, it is necessary to find the best to use library. We will use the BeautifulSoup (Python library), since it is easy and intuitive to work with: \n",
    "\n",
    "**BeautifulSoup**: It is an incredible tool for pulling out information from a webpage. You can use it to extract tables, lists, paragraph and you can also put filters to extract information from web pages. \n",
    "\n",
    "BeautifulSoup does not however fetch the web page for us. We will also use a library for opening the webpage URL:\n",
    "\n",
    "**Urllib.request**: It is a Python module which can be used for fetching URLs. \n",
    "\n",
    "Python has several other options for HTML scraping in addition to BeatifulSoup. Here are some others: mechanize, scrapemark, or scrapy\n",
    " \n",
    "\n",
    "## Basics – Get familiar with HTML (Tags)\n",
    "While scraping the we, you deal with html tags. Thus, it is quite useful to have good understanding of them.  Below is the basic syntax of HTML.  This syntax has various tags as elaborated below:\n",
    "\n",
    "    <!DOCTYPE html> : HTML documents must start with a type declaration\n",
    "    HTML document is contained between <html> and </html>\n",
    "    The visible part of the HTML document is between <body> and </body>\n",
    "    HTML headings are defined with the <h1> to <h6> tags\n",
    "    HTML paragraphs are defined with the <p> tag\n",
    "    Other useful HTML tags are:\n",
    "\n",
    "    HTML links are defined with the <a> tag, “<a href=“http://www.test.com”>This is a link for test.com</a>”\n",
    "    HTML tables are defined with<Table>, row as <tr> and rows are divided into data as <td>\n",
    "    html table\n",
    "    HTML list starts with <ul> (unordered) and <ol> (ordered). Each item of list starts with <li>\n",
    "    \n",
    "If you are new to this HTML tags, I would also recommend you to refer HTML tutorial from W3schools. This will give you a clear understanding about HTML tags.\n",
    "\n",
    "Before getting starting, take a look at the webpage you will be scraping:\n",
    "<a href=\"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\" target=\"_blank\">  https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India</a>\n",
    "\n",
    "Ok, now it is time to begin... you will have to import the **BeautifulSoup** library, which is part of the **bs4** package, and import **urllib.request**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the libraries now loaded, you will want to specific variables for webpage (i.e., URL) you are scraping. In this example we will call it \"wiki\", and use **urllib** to open that URL.  And then you will want to run **BeautifulSoup** on that \"page\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "page = urllib.request.urlopen(wiki)\n",
    "soup = BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the HTML version of the webpage using the \"soup.prettify()\" function.\n",
    "\n",
    "Exercise: In the cell below, write the code of printing (i.e., viewing) the HTML of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your results look like this at the top?\n",
    "\n",
    "    <!DOCTYPE html>\n",
    "    <html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
    "     <head>\n",
    "      <meta charset=\"utf-8\"/>\n",
    "      <title>\n",
    "       List of state and union territory capitals in India - Wikipedia\n",
    "      </title>\n",
    "      <script>\n",
    "       document.documentElement.className=\"client-js\"...\n",
    "\n",
    "If not, try again.  If you can't get it, highlight the lines below to view the correct code.\n",
    "\n",
    "<span style=\"color:white\"> print (soup.prettify())</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BeautifulSoup you scrape specific elements from the webpage using the HTML tags. You can bring in the whole element, including the tags, or just the contents found between the tags (in the example below case a \"string\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup can also be used to find specific HTML tags within a webpage. This is very helpful for finding the specific elements within the webpage that you want to scrape. This is when knowing HTML tags is useful, for example in HTML tables (such as the one we want to scrape from this Wikipedia page) are marked by \"table\" tags. So to find a table, we can use BeautifulSoup to locate and return the table(s) we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further the tables on the page by using other HTML tags. This allows us to get the specific table we want from the webpage. You can use the \"inspect\" function of your web browser to look at the HTML code for elements within the webpage as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_table=soup.find('table', class_='wikitable sortable plainrowheaders')\n",
    "right_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: In the cell below, write code that will retrieve just rows #2 and #3 of the table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your results look like this?\n",
    "\n",
    "    [<tr>\n",
    "    <td>2\n",
    "    </td>\n",
    "    <th scope=\"row\"><a href=\"/wiki/Andhra_Pradesh\" title=\"Andhra Pradesh\">Andhra Pradesh</a>\n",
    "    </th>\n",
    "    <td><a class=\"mw-redirect\" href=\"/wiki/Hyderabad,_India\" title=\"Hyderabad, India\">Hyderabad</a> <small>(<i>de jure</i> to 2024)</small><br/><a href=\"/wiki/Amaravati\" title=\"Amaravati\">Amaravati</a> <small>(<i>de facto</i> from 2017)</small><sup class=\"reference\" id=\"cite_ref-gulte.com_3-0\"><a href=\"#cite_note-gulte.com-3\">[3]</a></sup><sup class=\"reference\" id=\"cite_ref-4\"><a href=\"#cite_note-4\">[4]</a></sup><sup class=\"reference\" id=\"cite_ref-5\"><a href=\"#cite_note-5\">[a]</a></sup>\n",
    "    </td>\n",
    "    <td><a href=\"/wiki/Amaravati\" title=\"Amaravati\">Amaravati</a><sup class=\"reference\" id=\"cite_ref-gulte.com_3-1\"><a href=\"#cite_note-gulte.com-3\">[3]</a></sup>\n",
    "    </td>\n",
    "    <td><a href=\"/wiki/Andhra_Pradesh_High_Court\" title=\"Andhra Pradesh High Court\">Amaravati</a>\n",
    "    </td>\n",
    "    <td>1956<br/>2017\n",
    "    </td>\n",
    "    <td><a href=\"/wiki/Kurnool\" title=\"Kurnool\">Kurnool</a> (1953-1956)\n",
    "    </td></tr>, <tr>\n",
    "    <td>3\n",
    "    </td>\n",
    "    <th scope=\"row\"><a href=\"/wiki/Arunachal_Pradesh\" title=\"Arunachal Pradesh\">Arunachal Pradesh</a>\n",
    "    </th>\n",
    "    <td><a href=\"/wiki/Itanagar\" title=\"Itanagar\">Itanagar</a>\n",
    "    </td>\n",
    "    <td>Itanagar\n",
    "    </td>\n",
    "    <td><a href=\"/wiki/Guwahati\" title=\"Guwahati\">Guwahati</a>\n",
    "    </td>\n",
    "    <td>1986\n",
    "    </td>\n",
    "    <td> —\n",
    "    </td></tr>]\n",
    "    \n",
    "If not, try again.  If you can't get it, highlight the lines below to view the correct code.\n",
    "\n",
    "<span style=\"color:white\">\n",
    "table=soup.find('table', class_='wikitable sortable plainrowheaders')<br>\n",
    "allrows = table.find_all('tr')<br>\n",
    "print (allrows[2:4]) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retrieve all the links from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "for link in all_links:\n",
    "    print (link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retrieve just the last links on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')\n",
    "\n",
    "print('Total number of URLs present = ',len(all_links)) \n",
    "\n",
    "print('\\n\\nLast 5 URLs in the page are : \\n')\n",
    "\n",
    "if len(all_links) > 5 :\n",
    "  \n",
    "  last_5 = all_links[len(all_links)-5:]\n",
    "  for url in last_5 :\n",
    "    print(url.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with scraping the table data from the webpage, you can use organize the HTML table into a Python dataframe that you can work with for data analysis.   You will start by making each row into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate lists\n",
    "A=[]\n",
    "B=[]\n",
    "C=[]\n",
    "D=[]\n",
    "E=[]\n",
    "F=[]\n",
    "G=[]\n",
    "for row in right_table.findAll(\"tr\"):\n",
    "    cells = row.findAll('td')\n",
    "    states=row.findAll('th') #To store second column data\n",
    "    if len(cells)==6: #Only extract table body not heading\n",
    "        A.append(cells[0].find(text=True))\n",
    "        B.append(states[0].find(text=True))\n",
    "        C.append(cells[1].find(text=True))\n",
    "        D.append(cells[2].find(text=True))\n",
    "        E.append(cells[3].find(text=True))\n",
    "        F.append(cells[4].find(text=True))\n",
    "        G.append(cells[5].find(text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use pandas to make the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas to convert list to data frame\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(A,columns=['Number'])\n",
    "df['State/UT']=B\n",
    "df['Admin_Capital']=C\n",
    "df['Legislative_Capital']=D\n",
    "df['Judiciary_Capital']=E\n",
    "df['Year_Capital']=F\n",
    "df['Former_Capital']=G\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Now it is your turn to try one on your own.\n",
    "\n",
    "You want to scrape this wikipedia page: https://en.wikipedia.org/wiki/List_of_economic_expansions_in_the_United_States\n",
    "\n",
    "You want the table of growth periods since the Great Depression,  which can later use Pandas to analyze in interesting ways.\n",
    "\n",
    "In the cell below, write the Python code that will retrieve this table as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your results look similar to this?\n",
    "```text\n",
    "\tYears\tDuration\tAnnual Employement Growth\tAnnual GDP Growth\tDescription\n",
    "0\tOct 1945–\t37\t+5.2%\t+1.5%\tAs the United States demobilized from\n",
    "1\tOct 1949–\t45\t+4.4%\t+6.9%\tThe United States exited recession in late 194...\n",
    "2\tMay 1954–\t39\t+2.5%\t+4.0%\tExpansion resumed following a return to growth...\n",
    "3\tApril 1958–\t24\t+3.6%\t+5.6%\tA brief, two-year period of expansion occurred...\n",
    "4\tFeb 1961–\t106\t+3.3%\t+4.9%\tA long expansionary period began in 1961. Inco...\n",
    "5\tNov 1970–\t36\t+3.4%\t+5.1%\tGrowth resumed after the brief\n",
    "6\tMar 1975–\t58\t+3.6%\t+4.3%\tFollowing the steep\n",
    "7\tJul 1980–\t12\t+2.0%\t+4.4%\tThis short period of growth saw unemployment r...\n",
    "8\tDec 1982–\t92\t+2.8%\t+4.3%\tInflation was under control by the mid-1980s. ...\n",
    "9\tMar 1991–\t120\t+2.0%\t+3.6%\tFollowing a\n",
    "10\tNov 2001–\t73\t+0.9%\t+2.8%\tAnother mild recession\n",
    "11\tJune 2009–\t123+\\n\t+1.1%\t+2.3%\tThe effects of the\n",
    "```\n",
    "\n",
    "If not, try again. If you can't get it, highlight the lines below to view the correct code.<br>\n",
    "<font style=\"color:white\">\n",
    "wiki2 = \"https://en.wikipedia.org/wiki/List_of_economic_expansions_in_the_United_States\"  \n",
    "page2 = urllib.request.urlopen(wiki2)  \n",
    "soup2 = BeautifulSoup(page2)  \n",
    "table2 = soup2.find('table', class_='wikitable sortable')  \n",
    "A=[]  \n",
    "B=[]  \n",
    "C=[] \n",
    "D=[]  \n",
    "E=[]  \n",
    "F=[]  \n",
    "G=[]  \n",
    "for row in table2.find_all(\"tr\"):  \n",
    "    cells = row.find_all('td')  \n",
    "    dates=row.find_all('th') #To store second column data  \n",
    "    if len(cells)==5: #Only extract table body not heading  \n",
    "        A.append(cells[0].find(text=True))  \n",
    "        B.append(cells[1].find(text=True))  \n",
    "        C.append(cells[2].find(text=True))  \n",
    "        D.append(cells[3].find(text=True))  \n",
    "        E.append(cells[4].find(text=True))  \n",
    "df=pd.DataFrame(A,columns=['Years'])  \n",
    "df['Duration']=B  \n",
    "df['Annual Employement Growth']=C  \n",
    "df['Annual GDP Growth']=D  \n",
    "df['Description']=E  \n",
    "df  \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for completing the tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
